{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Regularized Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use loadmat to load matlab files\n",
    "mat=loadmat(\"ex5data1.mat\")\n",
    "\n",
    "# mat is a dict with key \"X\" for x-values, and key \"y\" for y values\n",
    "X=mat[\"X\"]\n",
    "y=mat[\"y\"]\n",
    "Xtest=mat[\"Xtest\"]\n",
    "ytest=mat[\"ytest\"]\n",
    "Xval=mat[\"Xval\"]\n",
    "yval=mat[\"yval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y,marker=\"x\",color=\"r\")\n",
    "plt.xlabel(\"Change in water level\")\n",
    "plt.ylim(0,40)\n",
    "plt.ylabel(\"Water flowing out of the dam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Regularized Linear Regression Cost Function\n",
    "\n",
    "$J(\\Theta) = \\frac{1}{2m} (\\sum_{i=1}^m(h_\\Theta(x^{(i)}) - y^{(i)})^2) + \\frac{\\lambda}{2m}(\\sum_{j=1}^n \\Theta_j^2)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegCostFunction(X, y,theta, Lambda):\n",
    "    \"\"\"\n",
    "    computes the cost of using theta as the parameter for linear regression to fit the data points in X and y. \n",
    "    \n",
    "    Returns the cost and the gradient\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    m = len(y)\n",
    "    hyposesis = np.dot(X,theta)\n",
    "    cost = 1/(2*m) * np.sum((hyposesis - y)**2)\n",
    "    \n",
    "########################\n",
    "#   Start of your code  #\n",
    "\n",
    "#   Compute the Regularized Linear Regression Cost Function\n",
    "    reg_cost = \n",
    "    \n",
    "#   compute the gradient\n",
    "    grad1 = \n",
    "    grad2 = \n",
    "\n",
    "#   end  of your code  #\n",
    "########################\n",
    "\n",
    "    grad = np.vstack((grad1[0],grad2[1:]))    \n",
    "    return reg_cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0]\n",
    "\n",
    "# Initiate theta as a (2,1) unit vector (np.zerose can be useful here)\n",
    "theta = np.ones((2,1))\n",
    "\n",
    "# Add a column of 1's to the training_set and name it X (np.append can be useful here)\n",
    "X_1 = np.hstack((np.ones((m,1)),X))\n",
    "\n",
    "# Calculate cost and grad by the using linearRegCostFunction defined above, for X_1 and y\n",
    "cost, grad = linearRegCostFunction(X_1, y, theta, 1)\n",
    "\n",
    "print(\"Cost at theta = [1 ; 1]:\",cost)\n",
    "print(\"Gradient at theta = [1 ; 1]:\",grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,num_iters,Lambda):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    m=len(y)\n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        cost, grad = linearRegCostFunction(X,y,theta,Lambda)\n",
    "        theta = theta - (alpha * grad)\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    return theta , J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 0\n",
    "theta, J_history = gradientDescent(X_1,y,np.zeros((2,1)),0.001,4000,Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting of Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(J_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\Theta)$\")\n",
    "plt.title(\"Cost function using Gradient Descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y,marker=\"x\",color=\"r\")\n",
    "plt.xlabel(\"Change in water level\")\n",
    "plt.ylabel(\"Water flowing out of the dam\")\n",
    "x_value=[x for x in range(-50,40)]\n",
    "y_value=[y*theta[1]+theta[0] for y in x_value]\n",
    "plt.plot(x_value,y_value,color=\"b\")\n",
    "plt.ylim(-5,40)\n",
    "plt.xlim(-50,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(X, y, Xval, yval, Lambda):\n",
    "    \"\"\"\n",
    "    Returns the train and cross validation set errors for a learning curve\n",
    "    \"\"\"\n",
    "    m=len(y)\n",
    "    n=X.shape[1]\n",
    "    err_train= [] \n",
    "    err_val = []\n",
    "    \n",
    "    for i in range(1,m+1):  \n",
    "########################\n",
    "#   Start of your code  #\n",
    "        # Use the gradientDescent() function difined above to find theta for training examples with different sizes.\n",
    "        theta = \n",
    "        \n",
    "        # Use the linearRegCostFunction() function and training set to calculate cost and append it to err_train\n",
    "        err_train.append()\n",
    "        \n",
    "        # Use the linearRegCostFunction() function and validation set to calculate cost and append it to err_val\n",
    "        err_val.append()\n",
    "\n",
    "#   end  of your code  #\n",
    "########################\n",
    "\n",
    "        \n",
    "    return err_train, err_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval_1 = np.hstack((np.ones((21,1)),Xval))\n",
    "\n",
    "########################\n",
    "#   Start of your code  #\n",
    "\n",
    "# Use the learningCurve() function to calculate err_train, err_val\n",
    "error_train, error_val = \n",
    "#   end  of your code  #\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(12),error_train,label=\"Train\")\n",
    "plt.plot(range(12),error_val,label=\"Cross Validation\",color=\"r\")\n",
    "plt.title(\"Learning Curve for Linear Regression\")\n",
    "plt.xlabel(\"Number of training examples\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# Training Examples\\t Train Error \\t\\t Cross Validation Error\")\n",
    "for i in range(1,13):\n",
    "    print(\"\\t\",i,\"\\t\\t\",error_train[i-1],\"\\t\",error_val[i-1],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyFeatures(X, p):\n",
    "    \"\"\"\n",
    "    Takes a data matrix X (size m x 1) and maps each example into its polynomial features where \n",
    "    X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ...  X(i).^p];\n",
    "    \"\"\"\n",
    "    for i in range(2,p+1):\n",
    "        X = np.hstack((X,(X[:,0]**i)[:,np.newaxis]))\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map X onto Polynomial features and normalize\n",
    "p=8\n",
    "\n",
    "########################\n",
    "#   Start of your code  #\n",
    "\n",
    "# Use the polyFeatures() defined above to map each training example into its polynomial features (p=8)\n",
    "X_poly = \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set the StandardScaler function of the sklearn to sc_X\n",
    "sc_X =\n",
    "\n",
    "# Use the fit_transform() module of StandardScaler and set it to X_poly\n",
    "X_poly = \n",
    "\n",
    "# Add a one column to the X_poly and asign it to X_poly\n",
    "X_poly = \n",
    "\n",
    "#   end  of your code  #\n",
    "########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Xtest onto polynomial features and normalize\n",
    "\n",
    "########################\n",
    "#   Start of your code  #\n",
    "\n",
    "# Use the polyFeatures() defined above to map each test example into its polynomial features (p=8)\n",
    "X_poly_test = \n",
    "\n",
    "# Set the transform function of the sklearn and set it to X_poly_test\n",
    "X_poly_test = \n",
    "\n",
    "# Add a one column to the X_poly and asign it to X_poly_test\n",
    "X_poly_test = \n",
    "\n",
    "#   end  of your code  #\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Xval onto polynomial features and normalize\n",
    "\n",
    "########################\n",
    "#   Start of your code  #\n",
    "\n",
    "# Use the polyFeatures() defined above to map each cross-validation example into its polynomial features (p=8)\n",
    "X_poly_val = \n",
    "\n",
    "# Set the transform function of the sklearn and set it to X_poly_val\n",
    "X_poly_val = \n",
    "\n",
    "# Add a one column to the X_poly and asign it to X_poly_val\n",
    "X_poly_val = \n",
    "\n",
    "#   end  of your code  #\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#   Start of your code  #\n",
    "\n",
    "# Use the gradientDescent() function, X_poly,y,and np.zeros((9,1))\n",
    "# to calculate theta_poly, J_history_poly\n",
    "theta_poly, J_history_poly = \n",
    "\n",
    "#   end  of your code  #\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By runing the following cell you can see how well the trained model works on CV set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(X,y,marker=\"x\",color=\"r\")\n",
    "plt.xlabel(\"Change in water level\")\n",
    "plt.ylabel(\"Water flowing out of the dam\")\n",
    "x_value=np.linspace(-55,65,2400)\n",
    "\n",
    "# Map the X values and normalize\n",
    "x_value_poly = polyFeatures(x_value[:,np.newaxis], p)\n",
    "x_value_poly = sc_X.transform(x_value_poly)\n",
    "x_value_poly = np.hstack((np.ones((x_value_poly.shape[0],1)),x_value_poly))\n",
    "y_value= x_value_poly @ theta_poly\n",
    "plt.plot(x_value,y_value,\"--\",color=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#   Start of your code  #\n",
    "\n",
    "# Use the learningCurve() function to calculate err_train, err_val\n",
    "error_train, error_val = \n",
    "\n",
    "#   end  of your code  #\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By runing the following cell to plot the learning cure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(12),error_train,label=\"Train\")\n",
    "plt.plot(range(12),error_val,label=\"Cross Validation\",color=\"r\")\n",
    "plt.title(\"Learning Curve for Linear Regression\")\n",
    "plt.xlabel(\"Number of training examples\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression with lambda = 100\n",
    "## By runing the following cell you can see how well the trained model works on CV set. In the follwing we are using a different lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 100\n",
    "########################\n",
    "#   Start of your code  #\n",
    "\n",
    "# Use the gradientDescent() function, X_poly,y,and np.zeros((9,1))\n",
    "# to calculate theta_poly, J_history_poly\n",
    "theta_poly, J_history_poly = \n",
    "\n",
    "#   end  of your code  #\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By runing the following cell you can see how well the trained model works on CV set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y,marker=\"x\",color=\"r\")\n",
    "plt.xlabel(\"Change in water level\")\n",
    "plt.ylabel(\"Water flowing out of the dam\")\n",
    "x_value=np.linspace(-55,65,2400)\n",
    "\n",
    "# Map the X values and normalize\n",
    "x_value_poly = polyFeatures(x_value[:,np.newaxis], p)\n",
    "x_value_poly = sc_X.transform(x_value_poly)\n",
    "x_value_poly = np.hstack((np.ones((x_value_poly.shape[0],1)),x_value_poly))\n",
    "y_value= x_value_poly @ theta_poly\n",
    "plt.plot(x_value,y_value,\"--\",color=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#   Start of your code  #\n",
    "\n",
    "# Use the learningCurve() function to calculate err_train, err_val\n",
    "error_train, error_val = \n",
    "\n",
    "#   end  of your code  #\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By runing the following cell to plot the learning cure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(12),error_train,label=\"Train\")\n",
    "plt.plot(range(12),error_val,label=\"Cross Validation\",color=\"r\")\n",
    "plt.title(\"Learning Curve for Linear Regression\")\n",
    "plt.xlabel(\"Number of training examples\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
